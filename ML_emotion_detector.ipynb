{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 08:43:03.202496: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING------\n",
      "7215: happy imgs\n",
      "4830: sad imgs\n",
      "4097: fear imgs\n",
      "3171: surprise imgs\n",
      "4965: neutral imgs\n",
      "3995: angry imgs\n",
      "436: disgust imgs\n",
      "TEST------\n",
      "1774: happy imgs\n",
      "1247: sad imgs\n",
      "1024: fear imgs\n",
      "831: surprise imgs\n",
      "1233: neutral imgs\n",
      "958: angry imgs\n",
      "111: disgust imgs\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING------\")\n",
    "for emotion in os.listdir('train/'):\n",
    "    print(str(len(os.listdir('train/' + emotion))) + ': ' + emotion + ' imgs')\n",
    "\n",
    "print(\"TEST------\")\n",
    "for emotion in os.listdir('test/'):\n",
    "    print(str(len(os.listdir('test/' + emotion))) + ': ' + emotion + ' imgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Set image and batch size\n",
    "image_size = (224,224)\n",
    "batch_size = 100\n",
    "\n",
    "# Define the directories for training and testing data\n",
    "train_data = 'train/'\n",
    "test_data = 'test/'\n",
    "\n",
    "# Create the image generator for normilization\n",
    "train_data_gen = ImageDataGenerator(horizontal_flip=True)\n",
    "test_data_gen = ImageDataGenerator(horizontal_flip=True)\n",
    "\n",
    "# Generate the batches of training and test data\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "    train_data,\n",
    "    target_size = image_size,\n",
    "    batch_size = batch_size,\n",
    "    color_mode = 'grayscale',\n",
    "    class_mode = 'categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_data_gen.flow_from_directory(\n",
    "    test_data,\n",
    "    target_size = image_size,\n",
    "    batch_size = batch_size,\n",
    "    color_mode = 'grayscale',\n",
    "    class_mode = 'categorical'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 222, 222, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 111, 111, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 54, 54, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 52, 52, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 26, 26, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 86528)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               11075712  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 903       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11169287 (42.61 MB)\n",
      "Trainable params: 11169287 (42.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(224,224, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(7, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "287/287 [==============================] - 4104s 14s/step - loss: 4.2696 - accuracy: 0.2875 - val_loss: 1.6523 - val_accuracy: 0.3583\n",
      "Epoch 2/10\n",
      "287/287 [==============================] - 3982s 14s/step - loss: 1.5769 - accuracy: 0.3883 - val_loss: 1.5606 - val_accuracy: 0.3934\n",
      "Epoch 3/10\n",
      "287/287 [==============================] - 3873s 13s/step - loss: 1.4367 - accuracy: 0.4501 - val_loss: 1.5150 - val_accuracy: 0.4254\n",
      "Epoch 4/10\n",
      "287/287 [==============================] - 3865s 13s/step - loss: 1.3173 - accuracy: 0.5032 - val_loss: 1.4484 - val_accuracy: 0.4492\n",
      "Epoch 5/10\n",
      "287/287 [==============================] - 3913s 14s/step - loss: 1.1992 - accuracy: 0.5563 - val_loss: 1.4619 - val_accuracy: 0.4538\n",
      "Epoch 6/10\n",
      "287/287 [==============================] - 4006s 14s/step - loss: 1.0734 - accuracy: 0.6066 - val_loss: 1.5375 - val_accuracy: 0.4630\n",
      "Epoch 7/10\n",
      "287/287 [==============================] - 3886s 14s/step - loss: 0.9486 - accuracy: 0.6617 - val_loss: 1.5919 - val_accuracy: 0.4583\n",
      "Epoch 8/10\n",
      "287/287 [==============================] - 4153s 14s/step - loss: 0.8263 - accuracy: 0.7123 - val_loss: 1.6782 - val_accuracy: 0.4670\n",
      "Epoch 9/10\n",
      "287/287 [==============================] - 4609s 16s/step - loss: 0.7123 - accuracy: 0.7596 - val_loss: 1.9269 - val_accuracy: 0.4483\n",
      "Epoch 10/10\n",
      "287/287 [==============================] - 4941s 17s/step - loss: 0.6124 - accuracy: 0.7932 - val_loss: 1.9815 - val_accuracy: 0.4508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x13de4c7d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(train_generator,\n",
    "          steps_per_epoch=train_generator.samples // batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=test_generator,\n",
    "          validation_steps=test_generator.samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 299s 4s/step - loss: 1.9805 - accuracy: 0.4521\n",
      "Test accuracy: 0.45211267471313477\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(test_generator, steps=test_generator.samples // batch_size)\n",
    "print(f'Test accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('output/emotion_detection.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the name of the last convolutional layer\n",
    "last_conv_layer_name = 'conv2d_2'\n",
    "\n",
    "# Load a sample image from your test dataset\n",
    "sample_image_path = 'test/fear/PrivateTest_623230.jpg'\n",
    "img = image.load_img(sample_image_path, target_size=(224, 224), color_mode='grayscale')\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# Define a function to generate CAM using OpenCV\n",
    "def generate_cam(model, last_conv_layer_name, image_path, target_size=(224, 224)):\n",
    "    # Load the image\n",
    "    img = image.load_img(image_path, target_size=target_size, color_mode='grayscale')\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    # Extract the last convolutional layer\n",
    "    last_conv_layer = model.get_layer(last_conv_layer_name)\n",
    "    \n",
    "    # Create a new model with the last convolutional layer as output\n",
    "    cam_model = Model(inputs=model.input, outputs=last_conv_layer.output)\n",
    "    \n",
    "    # Get the weights of the output layer\n",
    "    output_weights = model.layers[-1].get_weights()[0]\n",
    "    \n",
    "    # Compute the feature maps\n",
    "    features = cam_model.predict(img_array)\n",
    "    \n",
    "    # Calculate the class activation map\n",
    "    cam = np.matmul(features, output_weights)\n",
    "    cam = np.sum(cam, axis=-1)\n",
    "    \n",
    "    # Normalize CAM\n",
    "    cam /= np.max(cam)\n",
    "    \n",
    "    # Resize CAM to the size of the input image\n",
    "    cam = cv2.resize(cam[0], target_size)\n",
    "    \n",
    "    # Convert CAM to heatmap\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "    \n",
    "    # Convert grayscale image to BGR\n",
    "    img_bgr = cv2.cvtColor(np.uint8(img_array[0]), cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    # Overlay the heatmap on the original image\n",
    "    cam_overlay = cv2.addWeighted(heatmap, 0.5, img_bgr, 0.5, 0)\n",
    "    \n",
    "    return cam_overlay\n",
    "\n",
    "\n",
    "\n",
    "# Generate CAM using OpenCV\n",
    "cam_overlay = generate_cam(model, last_conv_layer_name, sample_image_path)\n",
    "\n",
    "\n",
    "# Generate CAM using OpenCV\n",
    "cam_overlay = generate_cam(model, last_conv_layer_name, sample_image_path)\n",
    "\n",
    "cv2.imwrite('output/cam_fear.jpg', cam_overlay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 798ms/step\n",
      "Predicted Emotion: happy\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('output/emotion_detection.h5')\n",
    "\n",
    "# Define the target size for resizing the images\n",
    "target_size = (224, 224)\n",
    "\n",
    "# Define a function to preprocess the image\n",
    "def preprocess_image(image_path):\n",
    "   # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    # Convert the image to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Resize the image to the target size\n",
    "    resized_image = cv2.resize(gray_image, target_size)\n",
    "    # Normalize the pixel values to be in the range [0, 1]\n",
    "    normalized_image = resized_image / 255.0\n",
    "    # Expand the dimensions of the image to match the input shape expected by the model\n",
    "    expanded_image = np.expand_dims(normalized_image, axis=-1)\n",
    "    # Return the preprocessed image\n",
    "    return expanded_image\n",
    "\n",
    "# Define a function to predict emotion from an image\n",
    "def predict_emotion(image_path):\n",
    "    # Preprocess the image\n",
    "    preprocessed_image = preprocess_image(image_path)\n",
    "    # Make prediction\n",
    "    predictions = model.predict(np.expand_dims(preprocessed_image, axis=0))\n",
    "    # Get the predicted emotion class\n",
    "    predicted_class = np.argmax(predictions)\n",
    "    # Map the predicted class to the corresponding emotion label\n",
    "    emotion_labels = ['happy', 'disgust', 'fear', 'angry', 'sad', 'surprised', 'neutral']\n",
    "    predicted_emotion = emotion_labels[predicted_class]\n",
    "    return predicted_emotion\n",
    "\n",
    "# Test the model on an example image\n",
    "image_path = 'test/happy/PrivateTest_556985.jpg'\n",
    "predicted_emotion = predict_emotion(image_path)\n",
    "print('Predicted Emotion:', predicted_emotion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 329s 5s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.13      0.16      0.14       958\n",
      "     disgust       0.00      0.00      0.00       111\n",
      "        fear       0.15      0.13      0.14      1024\n",
      "       happy       0.24      0.26      0.25      1774\n",
      "     neutral       0.17      0.14      0.15      1233\n",
      "         sad       0.19      0.20      0.19      1247\n",
      "    surprise       0.10      0.10      0.10       831\n",
      "\n",
      "    accuracy                           0.18      7178\n",
      "   macro avg       0.14      0.14      0.14      7178\n",
      "weighted avg       0.17      0.18      0.17      7178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate predictions for the test data\n",
    "y_pred = model.predict(test_generator)\n",
    "\n",
    "# Convert the predicted probabilities to class labels\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Get the true class labels from the test data generator\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Compute precision, recall, and F1-score\n",
    "report = classification_report(y_true, y_pred_classes, target_names=test_generator.class_indices.keys())\n",
    "\n",
    "# Print the classification report\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
